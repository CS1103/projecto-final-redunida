# Proyecto Final 2025-1: AI Neural Network

## **CS2013 Programaci√≥n III** ¬∑ Informe Final

### **Descripci√≥n**

Implementaci√≥n de una red neuronal multicapa en C++ desde cero, utilizando estructuras de datos personalizadas y principios de programaci√≥n orientada a objetos. El modelo es capaz de realizar tareas de clasificaci√≥n binaria usando funciones de activaci√≥n como ReLU y Sigmoid, funciones de p√©rdida como MSE y BCE, y optimizadores como SGD y Adam.

### Contenidos

1. [Datos generales](#datos-generales)
2. [Requisitos e instalaci√≥n](#requisitos-e-instalaci√≥n)
3. [Investigaci√≥n te√≥rica](#1-investigaci√≥n-te√≥rica)
4. [Dise√±o e implementaci√≥n](#2-dise√±o-e-implementaci√≥n)
5. [Ejecuci√≥n](#3-ejecuci√≥n)
6. [An√°lisis del rendimiento](#4-an√°lisis-del-rendimiento)
7. [Trabajo en equipo](#5-trabajo-en-equipo)
8. [Conclusiones](#6-conclusiones)
9. [Bibliograf√≠a](#7-bibliograf√≠a)
10. [Licencia](#licencia)

---

### Datos generales

- **Tema**: Redes Neuronales en AI
- **Grupo**: `RedUnida`
- **Integrantes**:
  - Diego Abraham Ladron de Guevara Aguirre ‚Äì 202410083 (Documentaci√≥n)
  - Ra√∫l Janampa Salvatierra - 202410411 (Responsable de investigaci√≥n te√≥rica)
  - Berr√∫ Tenorio, Andy Ra√≠ (Desarrollo de la arquitectura)
  - Demarini Leyton, Leandro Nadir (Implementaci√≥n del modelo)
  - Huaytalla Su√°rez, Andree Mauricio (Pruebas y benchmarking)

---

### Requisitos e instalaci√≥n

1. **Compilador**: GCC 11 o superior
2. **Dependencias**:
   - CMake 3.18+
   - No requiere librer√≠as externas (se implementa una clase `Tensor` propia)
3. **Instalaci√≥n**:
   ```bash
   git clone https://github.com/EJEMPLO/proyecto-final.git
   cd proyecto-final
   mkdir build && cd build
   cmake ..
   make
   ```

---

### 1. Investigaci√≥n te√≥rica

- **Objetivo**: Explorar fundamentos matem√°ticos y computacionales de redes neuronales artificiales.

#### Contenidos:

1. Historia y evoluci√≥n de las redes neuronales: desde el perceptr√≥n hasta el deep learning.
2. Arquitecturas cl√°sicas: MLP (utilizada en este proyecto), CNN y RNN.
3. Entrenamiento por descenso de gradiente (SGD, Adam) y retropropagaci√≥n.
4. Funciones de activaci√≥n (ReLU y Sigmoid) y p√©rdida (MSE, BCE).


## üìö Contenidos

### 1.1 Historia y Evoluci√≥n de las Redes Neuronales

Las redes neuronales artificiales surgieron en los a√±os 40 con el modelo de McCulloch y Pitts (1943). Posteriormente, el **perceptr√≥n** propuesto por Frank Rosenblatt (1958) marc√≥ el primer avance importante, aunque sus limitaciones llevaron al llamado "invierno de la IA" tras las cr√≠ticas de Minsky y Papert (1969).

En los a√±os 80, el desarrollo del algoritmo de **retropropagaci√≥n** permiti√≥ el entrenamiento de **perceptrones multicapa (MLP)**, reactivando la investigaci√≥n. Desde entonces, han surgido arquitecturas m√°s profundas y especializadas como las **CNN** y **RNN**, impulsando el auge del **deep learning** en m√∫ltiples disciplinas.

> üìñ Referencia: [1], [2]

---

### 1.2 Arquitecturas Cl√°sicas

- **MLP (Multilayer Perceptron)**: Capas completamente conectadas, ideales para clasificaci√≥n b√°sica y regresi√≥n.
- **CNN (Convolutional Neural Network)**: Utilizadas en tareas de visi√≥n por computadora. Detectan patrones locales mediante filtros.
- **RNN (Recurrent Neural Network)**: Adecuadas para datos secuenciales como texto o audio. Conservan memoria de corto plazo.

‚öôÔ∏è Este proyecto implementa una red del tipo **MLP**, utilizada para tareas de clasificaci√≥n supervisada.

> üìñ Referencia: [1], [4]

---

### 1.3 Entrenamiento: Descenso de Gradiente y Retropropagaci√≥n

El entrenamiento de una red neuronal consiste en minimizar una funci√≥n de p√©rdida usando algoritmos de optimizaci√≥n:

- **SGD (Stochastic Gradient Descent)**: Realiza actualizaciones con muestras individuales o mini-lotes.
- **Adam (Adaptive Moment Estimation)**: Variante m√°s robusta que ajusta din√°micamente los pasos de actualizaci√≥n.

La **retropropagaci√≥n** permite calcular los gradientes de manera eficiente utilizando la regla de la cadena.

> üìñ Referencia: [1], [3], [4]

---

### 1.4 Funciones de Activaci√≥n y P√©rdida

#### üîÅ Funciones de Activaci√≥n:
- **ReLU**: `f(x) = max(0, x)` ‚Üí R√°pida, evita saturaci√≥n para valores positivos.
- **Sigmoid**: `f(x) = 1 / (1 + e^{-x})` ‚Üí √ötil para salidas binarias.

#### üìâ Funciones de P√©rdida:
- **MSE (Mean Squared Error)**: Com√∫n en regresi√≥n.
- **BCE (Binary Cross-Entropy)**: Com√∫n en clasificaci√≥n binaria.

La elecci√≥n depende del tipo de salida y de la tarea que se desea resolver.

> üìñ Referencia: [2], [4]

---

### 2. Dise√±o e implementaci√≥n

#### 2.1 Arquitectura de la soluci√≥n

- **Patrones de dise√±o utilizados**:

  - Strategy: en la implementaci√≥n de optimizadores.
  - Interfaces puras para capas y funciones de p√©rdida.

- **Estructura de carpetas:**

```plaintext
proyecto-final/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ tensor.h              # Estructura principal para almacenamiento de datos
‚îÇ   ‚îú‚îÄ‚îÄ nn_interfaces.h       # Interfaces ILoss, ILayer, IOptimizer
‚îÇ   ‚îú‚îÄ‚îÄ nn_dense.h            # Implementaci√≥n de la capa densa
‚îÇ   ‚îú‚îÄ‚îÄ nn_activation.h       # Capas de activaci√≥n: ReLU, Sigmoid
‚îÇ   ‚îú‚îÄ‚îÄ nn_loss.h             # Funciones de p√©rdida: MSE, BCE
‚îÇ   ‚îú‚îÄ‚îÄ nn_optimizer.h        # Optimizadores: SGD y Adam
‚îÇ   ‚îú‚îÄ‚îÄ neural_network.h      # Clase de alto nivel para entrenamiento y predicci√≥n
‚îÇ   ‚îî‚îÄ‚îÄ main.cpp              # Ejemplo de uso
```

#### 2.2 Manual de uso y casos de prueba

- **C√≥mo ejecutar**:

```bash
./neural_net_demo input.csv output.csv
```

- **Casos de prueba**:
  - Capa densa: verificaci√≥n de forward y backward.
  - Activaciones: verificaci√≥n de ReLU y Sigmoid.
  - Entrenamiento sobre el dataset AND y XOR.
  - Comparaci√≥n de p√©rdida MSE vs BCE.

---

### 3. Ejecuci√≥n

> **Demo de ejemplo**: Video/demo alojado en `docs/demo.mp4`.

Pasos:

1. Preparar archivo CSV con datos de entrada y salida.
2. Ejecutar el programa principal con los par√°metros adecuados.
3. Evaluar los resultados impresos en consola o exportar a archivo.

---

### 4. An√°lisis del rendimiento

- **M√©tricas ejemplo** (con dataset XOR):

  - Iteraciones: 500 √©pocas.
  - Tiempo de entrenamiento: 4.8s.
  - Precisi√≥n final: 97.3%.

- **Ventajas/Desventajas**:

  -
    - C√≥digo ligero y comprensible.
  - ‚àí Sin paralelizaci√≥n ni soporte GPU.

- **Mejoras futuras**:

  - Integrar BLAS para acelerar la multiplicaci√≥n matricial.
  - A√±adir soporte para entrenamiento por lotes din√°micos.

---

### 5. Trabajo en equipo

| Tarea              | Miembro       | Rol                       |
| ------------------ |---------------|---------------------------|
| Investigaci√≥n te√≥rica | Diego         | Documentar bases te√≥ricas |
| Implementaci√≥n del modelo | Raul, Leandro | C√≥digo C++ de la NN       |
| Pruebas y benchmarking | Mauricio      | Generaci√≥n de m√©tricas    |
| Documentaci√≥n      | Andy          | Documentaci√≥n             |

---

### 6. Conclusiones

- **Logros**:

  - Se desarroll√≥ una red neuronal funcional en C++.
  - Se experimentaron m√∫ltiples combinaciones de activaciones, p√©rdidas y optimizadores.

- **Evaluaci√≥n**:

  - El sistema cumple los objetivos educativos planteados.

- **Aprendizajes**:

  - Profundizaci√≥n en backpropagation y arquitectura MLP.
  - Manejo de estructuras de datos complejas (tensores).

- **Recomendaciones**:

  - Mejorar la eficiencia computacional.
  - Explorar capas convolucionales y problemas reales como MNIST.

---

### 7. Bibliograf√≠a

[1] I. Goodfellow, Y. Bengio y A. Courville, *Deep Learning*, MIT Press, 2016. [Online]. Disponible: [https://www.deeplearningbook.org](https://www.deeplearningbook.org)

[2] M. Nielsen, "Neural Networks and Deep Learning", Determination Press, 2015. Disponible: [http://neuralnetworksanddeeplearning.com](http://neuralnetworksanddeeplearning.com)

[3] S. Ruder, "An overview of gradient descent optimization algorithms", arXiv preprint arXiv:1609.04747, 2016.

[4] F. Chollet, *Deep Learning with Python*, 2nd ed., Manning, 2021.

---

### Licencia

Este proyecto usa la licencia **MIT**. Ver [LICENSE](LICENSE) para detalles.

